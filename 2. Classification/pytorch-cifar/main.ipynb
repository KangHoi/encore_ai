{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"SxCZx5H5x36m"},"source":["### 해당 코드는 모든 인공지능 코드의 뼈대이자 핵심이다.\n","### 이것을 잘 마스터해두면 Segmentation 코드도 이해 가능하다. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4fqi0NB_bsj"},"source":["# Mount Google Drive Folder (/content/drive/My Drive/Colab)\n","# 1.Colab에서 구글 드라이버를 사용하기 위해 import 하는 부분\n","from google.colab import drive\n","\n","# 2.drive를 colab으로 mount해온다. drive의 하위로 위치가 잡혀야 한다. \n","drive.mount('/content/drive') \n","\n","# import user defined files \n","import sys\n","import_dir =\"/content/drive/My Drive/Colab Notebooks/2. Classification/pytorch-cifar\"\n","sys.path.insert(0, import_dir)\n","\n","# import ipynb files...\n","!pip install import_ipynb \n","import import_ipynb\n","\n","%run '/content/drive/My Drive/Colab Notebooks/2. Classification/pytorch-cifar/dataset.ipynb'\n","\n","import easydict \n","# 파이썬에서 실행 시 값 입력 python main.py --lr 0.01 -- resume False.. 이런식으로 들어간다\n","# ipynb은 라인기반으로 작동하므로 위와 같이 값을 넣을 수 없음.. 이를 위해 easydict 사용\n","# argparse.. 를 colab에서 사용 불가. easydict는 args형태로 변경해주는 역할\n","args = easydict.EasyDict({ \"lr\": 0.1, \"resume\": False}) \n","\n","# Debugging Tool\n","import pdb\n","\n","# Download & Unzip Dataset (Move from google drive to local)\n","# 3.Google Drive에 있는 cifar.tgz 압축파일을 복사해 colab의 로컬(content/sample_data) 이 안으로 저장해오겠다.\n","# 절대경로로 잡음. /content/drive/ 아래는 모두 구글 drive 부분\n","%cp -r '/content/drive/My Drive/Colab Notebooks/Dataset/CIFAR10/cifar.tgz' '/content/sample_data'\n","\n","# 4. colab의 로컬 위치에서 (content/sample_data) 이 안에서 압축을 풀었다.\n","!tar -xvf  '/content/sample_data/cifar.tgz'\n","\n","# Initilize Additional parameters\n","train_root = \"/content/cifar/train\"\n","test_root = \"/content/cifar/test\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q7zh5csGSS8"},"source":["'''Train CIFAR10 with PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","\n","'''\n","from models import * \n","models 패키지에 있는 모든 것을 다 가져와라... *의 의미\n","models 안으로 들어가면 여러개의 파일이 있다.\n","1. __init__.py 를 제일 먼저 찾게 되어있다. 가장 먼저 import 된다.\n","2. __init__.py 이 안으로 들어가면\n","    우리가 선택해 사용할 수 있는 여러 모델들이 나와있다.\n","    모델들이 import 되어진다.\n","\n","결론은 __init__이 먼저 import 되고 __init__에 들어있는 것들이 나중에 import되어지는\n","이중적인 구조로 되어있다.\n","'''\n","from models import * # models의 모든 것을 호출하여 사용\n","from utils import progress_bar # utils를 호출하여 사용\n","\n","'''\n","# Removed Code (Because ipynb file does not support argparse)\n","# Even removed, these lines are substitued by easydict code\n","\n","parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n","parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n","parser.add_argument('--resume', '-r', action='store_true',\n","                    help='resume from checkpoint')\n","args = parser.parse_args()\n","'''\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # 사용 device가 gpu인지 cpu인지 확인\n","best_acc = 0  # best test accuracy.. 체크포인트보다 더 클 경우 받아올 변수\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","\n","#데이터 변형.. 전처리 과정..Data Augmentation\n","transform_train = transforms.Compose([ \n","    # 패딩으로 40*40가 된 이미지를 랜덤으로 잘라 사이즈 크기(32)로 출력.. \n","    # padding을 주고 crop이 될 경우 padding이 적용된 부분은 0으로 보인다. 따라서 darkness, crop 둘 다 적용\n","    transforms.RandomCrop(32, padding=4), \n","    transforms.RandomHorizontalFlip(), # 이미지를 랜덤으로 수평으로 뒤집는다.(좌우반전)\n","    transforms.ToTensor() # Tensor형으로 전환\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor() # 테스트데이터를 Tensor형으로 전환\n","])\n","\n","# Modified for custom dataset\n","#trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","# dataset.ipynb에서 호출된다.\n","trainset = Dataset(root=train_root, transforms=transform_train) # 직접경로를 설정하여 train data를 transform형식으로 불러온다...로컬에 있는 것을 받아옴.. 튜닝할 때 사용\n","trainloader = torch.utils.data.DataLoader( # 배치사이즈 형태로 만들어서 실제로 학습할때 이용할 수 있는 형태로 만든다.. 128개로 쪼개서 data load시마다 load됨.\n","    trainset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), drop_last=True) # drop_last는 기본이 False,, 실제로는 거의 True. 나머지는 버리겠다.\n","\n","# Modified for custom dataset\n","#testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testset = Dataset(root=test_root, transforms=transform_test) # 직접경로를 설정하여 test data를 transform형식으로 불러온다...튜닝할 때 사용\n","testloader = torch.utils.data.DataLoader( # 배치사이즈 형태로 만들어서 실제로 학습할때 이용할 수 있는 형태로 만든다.\n","    testset, batch_size=100, shuffle=False, num_workers=os.cpu_count(), drop_last=True)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck') # labels\n","\n","print('==> Building model..')\n","# net = VGG('VGG19')\n","# net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","# net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","net = MobileNet() # 가장 가벼운 모델\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","# net = ShuffleNetV2(1)\n","# net = EfficientNetB0()\n","# net = RegNetX_200MF()\n","\n","\n","net = net.to(device)\n","\n","# 우리는 gpu서버를 한대 돌리므로 필요 없지만.. 여러대 돌릴 경우 아래 줄 필수이다!\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net) # DataParallel..모델을 병렬로 실행하여 다수의 gpu에서 사용 가능하다.\n","    cudnn.benchmark = True # 내장된 cudnn 자동 튜너를 활성화하여, 하드웨어에 맞게 사용할 최상의 알고리즘을 찾는다.\n","'''\n","가장 좋은 결과를 저장하여 불가피하게 중지되었을 경우 처음부터 학습이 진행되는 것이 아니라\n","가장 좋은 결과가 도출되었을 때부터 다시 학습시킨다. 이를 위해 true로 바꾸어 학습하면\n","가장 좋은 결과가 나왔을 때 부터 학습이 이어서 시작된다.\n","\n","처음에는 결과가 존재하지 않으므로 false로 두어야 오류가 나지 않는다.\n","'''\n","if args.resume: #처음에 false로 지정되어있다.. 그 후 true로 바꾸어 돌린다..\n","    # Load checkpoint.\n","    print('==> Resuming from checkpoint..')   # 데이터를 효율적으로 실행시키기위해 checkpoint를 지정해 중간중간 저장..\n","    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'  # assert.. 디버깅.. checkpoint가 없으면 error 띄운다\n","    checkpoint = torch.load('./checkpoint/ckpt.pth') # 저장된 객체 파일들을 역직렬화하여 메모리에 올립니다. \n","    \n","    net.load_state_dict(checkpoint['net']) # 역직렬화된 state_dict 를 사용하여 모델의 매개변수들을 불러옵니다\n","    best_acc = checkpoint['acc'] # checkpoint에 저장된 best_acc를 받아온다.\n","    start_epoch = checkpoint['epoch'] # checkpoint에 저장된 start_epoch을 받아온다.\n","\n","criterion = nn.CrossEntropyLoss() # 손실함수 crossentropy 사용\n","optimizer = optim.SGD(net.parameters(), lr=args.lr,\n","                      momentum=0.9, weight_decay=5e-4) # SGD사용\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0    \n","\n","'''\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        trainloader가 5만개라면 image label이 100개씩 압축되어져 할당\n","        이는 곧 100번 호출되었다는 얘기와 같다. 호출 시 dataloader가 해주는 역할\n","\n","'''\n","    for batch_idx, (inputs, targets) in enumerate(trainloader): # trainloader의 데이터를 enumerate함수를 활용해 인덱스를 지정\n","        inputs, targets = inputs.to(device), targets.to(device) # input과 target을 gpu로 사용할 수 있도록\n","        optimizer.zero_grad() # 처음 시작을 0으로 하기위해\n","        outputs = net(inputs) # 사용하는 모델에 inputs를 넣어 outputs 도출\n","        loss = criterion(outputs, targets) # 도출된 outputs를 손실함수에 적용하여 target과 비교 후, loss값 도출\n","        loss.backward() # loss 값에 대한 변화도를 전달받고 모델의 매개변수들에 대한 손실의 변화도 계산\n","        optimizer.step() #step 함수를 호출하면 매개변수 갱신\n","        train_loss += loss.item() # loss가 가진 값이 스칼라 형태이므로 item()를 사용해 train_loss에 더해준다.\n","        _, predicted = outputs.max(1) # outputs를 max로 정렬해 첫 번째 즉, 가장 큰 값을 predicted에 할당   \n","        total += targets.size(0) # target의 전체 개수를 total에 더해준다.\n","        correct += predicted.eq(targets).sum().item() # 예측값이 target값과 같을 경우 sum()으로 합쳐 correct에 더해줌.\n","\n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' \n","                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total)) # 결과값 출력\n","        #print(batch_idx,'/', len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","        #             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):  # validation test../ 최저의 로스만 저장한다. 가장 최적만을 저장. 나머지는 저장하지 않음\n","    global best_acc #전역변수로 지정\n","    net.eval() \n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            # 딥러닝은 후처리도 중요하다.. 따라서 출력이 잘 되어야할 필요도 존재\n","            # progress_bar.. utils에서 불러서 사용\n","            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","            #print(batch_idx,'/', len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","            #             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.   \n","    acc = 100.*correct/total\n","    if acc > best_acc: # acc가 best_acc보다 크다면\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(), # 처음 돌 때는 무조건 weight 값 저장\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint') # 없으면 directory 생성\n","        torch.save(state, './checkpoint/ckpt.pth') # 체크포인트를 저장\n","        best_acc = acc\n","\n","\n","\n","# 호출이 여기서 일어난다..\n","for epoch in range(start_epoch, start_epoch+20):\n","    train(epoch) # train먼저 돌고\n","    test(epoch) #test가 돌고 test function 내부에서 savepoint가 실행된다. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6zhoF8JVYSY"},"source":[""],"execution_count":null,"outputs":[]}]}