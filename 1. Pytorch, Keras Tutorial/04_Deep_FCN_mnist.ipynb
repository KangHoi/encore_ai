{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_Deep_FCN_mnist.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPihn6VCCrvrjrHAs6WxTBF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"e1ryRRQnN0oB"},"source":["import torch\n","import torchvision\n","import torch.nn as nn \n","import numpy as np\n","import torchvision.transforms as transforms \n","import matplotlib.pyplot as plt\n","\n","#우리가 사용할 컴퓨터를 check하는 부분, cpu/gpu 지원받을지..\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5j7lSfFGOC3w"},"source":["# 하이퍼파라미터 설정\n","\n","input_size = 784 # mnist는 28x28x1로 구성\n","hidden_size = 500 # 은닉층의 unit 수\n","num_classes = 10 # 카테고리 개수\n","num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# dataset 로딩 - 2개의 과정을 거친다\n","# train_dataset, test_dataset 데이터 로딩 1단계\n","train_dataset = torchvision.datasets.MNIST(root='../../data',\n","                                           train = True, #traindataset만 저장\n","                                           transform = transforms.ToTensor(),\n","                                           download = True) #traindataset을 스케일링하여 다운로드하겠다.\n","test_dataset = torchvision.datasets.MNIST(root='../../data',\n","                                           train = False, #test_dataset만 저장, test옵션은 따로 없음.\n","                                           transform = transforms.ToTensor())\n","\n","# 데이터 로딩 2단계.. BatchSize 이용\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size = batch_size,\n","                                           shuffle =True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                           batch_size = batch_size,\n","                                           shuffle =False) #테스트는 학습이아니므로 shuffle 필요 없다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9pvvm5YW4p_"},"source":["#### Model 생성\n"]},{"cell_type":"code","metadata":{"id":"I3b0bVI4OC1L"},"source":["class NeuralNet(nn.Module): #nn.Module을 상속받아 NeuralNet이라는 클래스 생성\n","    # 모델 설계\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) # Linear : Wx+b이므로\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","\n","    # 모델의 Forward Path 정의\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","\n","        return out #여기까지 클래스 정의 부분\n","\n","# 위에서 정의한 클래스를 인스턴스화 시킴\n","model = NeuralNet(input_size, hidden_size, num_classes).to(device) # to(device).. 이 모델을 gpu서버에서 돌리겠다.\n","\n","# loss and optimizer를 선정의\n","loss_function = nn.CrossEntropyLoss() # Loss 기능 안에 Softmax(예측값 극대화 후 확률값으로 변환)함수 기능 포함되어져 있음\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","total_step=len(train_loader) # 600번 로딩.. 600번 학습 진행\n","\n","for epoch in range(num_epochs): # 5번...\n","    for i,(images, labels) in enumerate(train_loader): # 한번에 100개씩 총 600번 들어가고 그게 5번(num_epochs) 반복!!!\n","        #네트워크에 넣어줄때는 1차원으로 펼쳐서 넣는다.\n","        images = images.reshape(-1, 28*28).to(device) #-1은 나머지 즉, 채널을 의미, gpu로 돌린다.\n","        labels = labels.to(device)\n","\n","        # Forward Pass\n","        pred = model(images)\n","        loss = loss_function(pred, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if(i+1) % 100 ==0:\n","            print('Epoch[{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9xxNlPmOCyP"},"source":["# Test\n","'''\n","1) with torch.no_grad():\n","    some code\n","    --> backward를 하지 않겠다.\n","\n","2) with torch.grad_enabled(False):\n","\n","만약 이 구문을 하지 않으면\n","기본적으로 BackPropagation을 진행할 것으로 알고 이에 해당하는 \n","메모리를 따로 빼둔다.\n","\n","결과적으로 이 구문을 작성하지 않으면 메모리를 더 많이 먹는다.\n","\n","\n","'''\n","with torch.no_grad(): # 실제로 학습할 필요 없을 때 이 구문을 반드시 작성\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","        '''\n","        max는 기본적으로 2개의 리턴 값을 가짐\n","            - 결과에 해당하는 값 \n","            - 값에 해당하는 인덱스 리턴\n","        _(언더바)로 변수를 받을 경우 값을 리턴 받겠지만 필요없으므로 값을 저장하지는 않겠다.\n","        1을 사용하지 않으면 디폴트는 0인데, 디폴트로 그냥 둘 경우\n","        세로로 값을 비교한다. 가로값과 비교하기위해 1을 사용한다. \n","        '''\n","        _,predicted = torch.max(outputs.data, 1)\n","\n","        total +=labels.size(0)\n","        correct +=(predicted == labels).sum().item()\n","    \n","\n","    print('Accuracy of the Network on the Test Images :{} %'.format(100 * correct/total))\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[]}]}